# ğŸ—ºï¸ Google Place Extractor

A powerful **Google Places scraper** with a real-time interactive map interface. Extract business data from **anywhere in the world** using an adaptive grid system that automatically handles dense areas.

![Screenshot](screenshot.png?v=2)

## âœ¨ Features

- **ğŸŒ Worldwide Coverage** â€“ Works in any country, any city
- **ğŸ—ºï¸ Interactive Map** â€“ Click to position your search grid anywhere
- **ğŸ“ Preset Locations** â€“ Quick access to major cities (Paris, New York, London, Tokyo, etc.)
- **ğŸ”„ Adaptive Grid System** â€“ Automatically subdivides dense areas to capture all results
- **âš¡ Parallel Processing** â€“ Multi-threaded API calls with configurable RPS
- **ğŸ“Š Live Progress** â€“ Real-time visualization of processed zones and found places
- **ğŸ’¾ CSV Export** â€“ Save results with full Google Places data

## ğŸš€ Quick Start

### 1. Clone the repository
```bash
git clone https://github.com/Blurenis/Google_Place_Extractor.git
cd Google_Place_Extractor
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Configure your API key
Create a `.env` file:
```env
GOOGLE_KEY=your_google_places_api_key
```

### 4. Run the app
```bash
streamlit run main.py
```

## ğŸ“‹ Usage

1. **Select a location** â€“ Choose a preset city or click anywhere on the map
2. **Set parameters** â€“ Configure keyword, grid size, and request speed
3. **Start scraping** â€“ Use "Pas Ã  Pas" for batch mode or enable "Auto-Run" for continuous scraping
4. **Export data** â€“ Click "Sauvegarder" to export results to CSV

## âš™ï¸ Configuration

| Parameter | Description | Default |
|-----------|-------------|---------|
| **Mot-clÃ©** | Search keyword (e.g., "restaurant", "dentist") | `infirmier libÃ©ral` |
| **Rayon Min** | Minimum search radius in meters | `100` |
| **RequÃªtes/Seconde** | API calls per batch | `2` |
| **Taille Grille** | Initial grid size (N Ã— 70km blocks) | `3` |

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py          # Streamlit app & scraping logic
â”œâ”€â”€ utils.py         # API calls, geometry helpers, CSV handling
â”œâ”€â”€ .env             # API key configuration (create this)
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md
```

## ğŸ”§ How It Works

1. **Grid Generation** â€“ Creates an NÃ—N grid of 70km blocks centered on your selected location
2. **Smart Subdivision** â€“ If a zone returns 20+ results (API limit), it splits into 4 sub-quadrants
3. **Dense Area Handling** â€“ For small zones still hitting 20+ results, fetches up to 3 pages (60 results max)
4. **Deduplication** â€“ Results are deduplicated by `place_id` when saved

## ğŸ“Š Output Format

The CSV export includes:
- `name` â€“ Business name
- `place_id` â€“ Unique Google identifier
- `formatted_address` â€“ Full address
- `geometry.location.lat/lng` â€“ Coordinates
- `rating` â€“ Average rating
- `user_ratings_total` â€“ Number of reviews
- `types` â€“ Business categories
- And more...

## âš ï¸ Important Notes

- **API Costs** â€“ Google Places API has costs. Monitor your usage in Google Cloud Console.
- **Rate Limiting** â€“ Default 2 RPS is conservative. Increase carefully to avoid quota issues.
- **Terms of Service** â€“ Ensure compliance with Google's ToS for your use case.

## ğŸ› ï¸ Requirements

- Python 3.8+
- Google Cloud account with Places API enabled
- Valid API key with billing configured

## ğŸ“„ License

MIT License â€“ Free to use and modify.

---

Made with â¤ï¸ using [Streamlit](https://streamlit.io/) & [Folium](https://python-visualization.github.io/folium/)

