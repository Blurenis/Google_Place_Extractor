# ğŸ—ºï¸ DOM Scraper Live

A powerful **Google Places scraper** with a real-time interactive map interface, built with Streamlit and Folium. Designed specifically for scraping business data across French Overseas Territories (DOM-TOM).

![Screenshot](screenshot.png)

## âœ¨ Features

- **ğŸ—ºï¸ Interactive Map** â€“ Click to position your search grid anywhere on the map
- **ğŸ“ Pre-configured DOM Zones** â€“ Quick access to Martinique, Guadeloupe, La RÃ©union, Mayotte, Guyane, Saint-BarthÃ©lemy & Saint-Martin
- **ğŸ”„ Adaptive Grid System** â€“ Automatically subdivides dense areas to capture all results
- **âš¡ Parallel Processing** â€“ Multi-threaded API calls with configurable RPS (requests per second)
- **ğŸ“Š Live Progress** â€“ Real-time visualization of processed zones and found places
- **ğŸ’¾ CSV Export** â€“ Save results with full Google Places data (name, address, coordinates, ratings, etc.)

## ğŸš€ Quick Start

### 1. Clone the repository
```bash
git clone https://github.com/YOUR_USERNAME/dom-scraper-live.git
cd dom-scraper-live
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Configure your API key
Create a `.env` file:
```env
GOOGLE_KEY=your_google_places_api_key
```

### 4. Run the app
```bash
streamlit run main.py
```

## ğŸ“‹ Usage

1. **Select a zone** â€“ Choose a DOM region from the dropdown or click on the map
2. **Set parameters** â€“ Configure keyword, grid size, and request speed
3. **Start scraping** â€“ Use "Pas Ã  Pas" for batch mode or enable "Auto-Run" for continuous scraping
4. **Export data** â€“ Click "Sauvegarder" to export results to CSV

## âš™ï¸ Configuration

| Parameter | Description | Default |
|-----------|-------------|---------|
| **Mot-clÃ©** | Search keyword | `infirmier libÃ©ral` |
| **Rayon Min** | Minimum search radius (m) | `100` |
| **RequÃªtes/Seconde** | API calls per batch | `2` |
| **Taille Grille** | Initial grid size (N Ã— 70km blocks) | `3` |

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py          # Streamlit app & scraping logic
â”œâ”€â”€ utils.py         # API calls, geometry helpers, CSV handling
â”œâ”€â”€ .env             # API key configuration
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md        # This file
```

## ğŸ”§ How It Works

1. **Grid Generation** â€“ Creates an NÃ—N grid of 70km blocks centered on your selected location
2. **Smart Subdivision** â€“ If a zone returns 20+ results, it splits into 4 sub-quadrants
3. **Dense Area Handling** â€“ For small zones still hitting 20+ results, fetches up to 3 pages (60 results)
4. **Deduplication** â€“ Results are deduplicated by `place_id` when saved

## ğŸ“Š Output Format

The CSV export includes:
- `name` â€“ Business name
- `place_id` â€“ Unique Google identifier
- `formatted_address` â€“ Full address
- `geometry.location.lat/lng` â€“ Coordinates
- `rating` â€“ Average rating
- `user_ratings_total` â€“ Number of reviews
- `types` â€“ Business categories
- `source_sector_id` â€“ Sector that found this result

## âš ï¸ Important Notes

- **API Costs** â€“ This tool makes Google Places API calls. Monitor your usage to avoid unexpected charges.
- **Rate Limiting** â€“ Respect Google's quotas. The default 2 RPS is conservative.
- **For Educational/Research Use** â€“ Ensure compliance with Google's Terms of Service.

## ğŸ› ï¸ Requirements

- Python 3.8+
- Google Cloud account with Places API enabled
- Valid API key with billing configured

## ğŸ“„ License

MIT License â€“ Feel free to use and modify for your projects.

---

Made with â¤ï¸ using [Streamlit](https://streamlit.io/) & [Folium](https://python-visualization.github.io/folium/)
